{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "_SKOsHpA-_9s",
        "outputId": "b70448fe-c571-4b74-c2a7-a5e9081addef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character-level model built from corpus.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://388e26b81269bcbb8c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://388e26b81269bcbb8c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "# --- SAMPLE TRAINING DATA ---\n",
        "# A small corpus to build our probabilistic model from.\n",
        "corpus = \"\"\"\n",
        "    In the heart of a dense, ancient forest, where sunlight struggled to pierce the thick canopy, lived a creature of myth and legend.\n",
        "    This was no ordinary beast, but a griffin, with the body of a lion and the head and wings of an eagle.\n",
        "    Its name was Ignis, and its feathers shimmered with the colors of a sunset.\n",
        "    Ignis was a guardian, a protector of the forest's deepest secrets.\n",
        "    One of these secrets was the Moonpetal flower, a bloom that only opened under the light of a full moon and held the power to heal any ailment.\n",
        "    Many had sought the Moonpetal, but the forest's winding paths and the griffin's watchful eyes kept it safe.\n",
        "    A young herbalist named Elara, however, was different. She came not with greed, but with a desperate plea.\n",
        "    Her village was struck by a mysterious illness, and the Moonpetal was their only hope.\n",
        "    She journeyed for days, her resolve unwavering. When she finally reached the forest's heart, she did not draw a weapon.\n",
        "    Instead, she offered a simple songbird's feather, a token of peace.\n",
        "    Ignis, who had seen countless intruders, was intrigued. The griffin listened as Elara spoke of her village's plight.\n",
        "    Moved by her sincerity, Ignis decided to trust her. The majestic creature led her to a hidden grove where the Moonpetals glowed with an ethereal light.\n",
        "    Elara took only a single bloom, thanking the griffin with a promise to protect the secret.\n",
        "    She returned to her village, and the flower's magic worked. The illness vanished.\n",
        "    The legend of the girl and the griffin became a whispered tale, a reminder that courage and compassion can be more powerful than any sword.\n",
        "\"\"\".strip().lower()\n",
        "\n",
        "# --- GLOBAL VARIABLES FOR THE MODEL ---\n",
        "char_to_index = {}\n",
        "index_to_char = {}\n",
        "vocab_size = 0\n",
        "transitions = {}\n",
        "\n",
        "# --- MODELING LOGIC ---\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"Preprocesses the corpus and builds the transition probability model.\"\"\"\n",
        "    global char_to_index, index_to_char, vocab_size, transitions\n",
        "\n",
        "    chars = sorted(list(set(corpus)))\n",
        "    vocab_size = len(chars)\n",
        "    char_to_index = {ch: i for i, ch in enumerate(chars)}\n",
        "    index_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Build a frequency map of next characters\n",
        "    freq_map = {}\n",
        "    for i in range(len(corpus) - 1):\n",
        "        char = corpus[i]\n",
        "        next_char = corpus[i + 1]\n",
        "        if char not in freq_map:\n",
        "            freq_map[char] = {}\n",
        "        if next_char not in freq_map[char]:\n",
        "            freq_map[char][next_char] = 0\n",
        "        freq_map[char][next_char] += 1\n",
        "\n",
        "    # Convert frequencies to probabilities\n",
        "    transitions = {}\n",
        "    for char, next_chars in freq_map.items():\n",
        "        transitions[char] = {}\n",
        "        total = sum(next_chars.values())\n",
        "        for next_char, count in next_chars.items():\n",
        "            transitions[char][next_char] = count / total\n",
        "\n",
        "def sample(prob_dist):\n",
        "    \"\"\"\n",
        "    Predicts the next character based on a probability distribution.\n",
        "    prob_dist is a dictionary like {'a': 0.5, 'b': 0.5}\n",
        "    \"\"\"\n",
        "    rand = random.random()\n",
        "    cumulative_prob = 0\n",
        "    # Sort items to ensure consistent order for sampling\n",
        "    for char, prob in sorted(prob_dist.items()):\n",
        "        cumulative_prob += prob\n",
        "        if rand < cumulative_prob:\n",
        "            return char\n",
        "    return sorted(prob_dist.keys())[-1] # Fallback\n",
        "\n",
        "def generate_text(model_type, seed_text, length):\n",
        "    \"\"\"\n",
        "    Simulates text generation using either RNN or LSTM logic.\n",
        "    \"\"\"\n",
        "    if not seed_text:\n",
        "        return \"Error: Please provide some seed text to start.\"\n",
        "\n",
        "    generated_text = seed_text\n",
        "    context = seed_text.lower()\n",
        "\n",
        "    for _ in range(int(length)):\n",
        "        # --- MODEL LOGIC SIMULATION ---\n",
        "        if model_type == 'Simple RNN':\n",
        "            # Simple RNN: Memory is short, only considers the last character.\n",
        "            context_len = 1\n",
        "            recent_context = context[-context_len:]\n",
        "            last_char = recent_context[-1]\n",
        "        else: # LSTM\n",
        "            # LSTM: Simulates longer memory by looking for a valid context char\n",
        "            # in a larger window.\n",
        "            context_len = 10\n",
        "            recent_context = context[-context_len:]\n",
        "            # Find the most relevant character from the recent context that exists in our model\n",
        "            last_char = next((char for char in reversed(recent_context) if char in transitions), context[-1])\n",
        "\n",
        "        # Get the probability distribution for the next character\n",
        "        next_char_probs = transitions.get(last_char)\n",
        "\n",
        "        if not next_char_probs:\n",
        "            # If we have no data for this char, pick a random one from our vocabulary\n",
        "            next_char = random.choice(list(char_to_index.keys()))\n",
        "        else:\n",
        "            next_char = sample(next_char_probs)\n",
        "\n",
        "        generated_text += next_char\n",
        "        context += next_char\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# --- BUILD THE MODEL ON STARTUP ---\n",
        "build_model()\n",
        "print(\"Character-level model built from corpus.\")\n",
        "\n",
        "# --- GRADIO INTERFACE ---\n",
        "with gr.Blocks(theme=gr.themes.Glass(), title=\"Text Generation\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center;\">\n",
        "            <h1 style=\"color: white;\">Character-Level Text Generation ✍️</h1>\n",
        "            <p style=\"color: #E5E7EB;\">Generate text using a simple RNN or an LSTM model. Select a model, provide seed text, and click Generate.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            model_type_input = gr.Radio(\n",
        "                [\"Simple RNN\", \"LSTM\"],\n",
        "                label=\"Select Model\",\n",
        "                value=\"LSTM\"\n",
        "            )\n",
        "            seed_text_input = gr.Textbox(\n",
        "                label=\"Seed Text\",\n",
        "                value=\"The ancient forest\",\n",
        "                lines=3,\n",
        "                placeholder=\"Enter your starting text here...\"\n",
        "            )\n",
        "            length_input = gr.Slider(\n",
        "                50, 1000,\n",
        "                value=300,\n",
        "                step=10,\n",
        "                label=\"Length of Generated Text\"\n",
        "            )\n",
        "            generate_btn = gr.Button(\"Generate Text\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            output_text = gr.Textbox(\n",
        "                label=\"Generated Output\",\n",
        "                lines=16,\n",
        "                placeholder=\"Your generated text will appear here...\"\n",
        "            )\n",
        "\n",
        "    generate_btn.click(\n",
        "        fn=generate_text,\n",
        "        inputs=[model_type_input, seed_text_input, length_input],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WD_HzZUx_Gtw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}